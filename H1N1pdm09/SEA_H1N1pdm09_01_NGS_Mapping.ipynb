{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGS mapping  \n",
    "  \n",
    "This jupyter notebook maps the raw FASTQ sequencing reads of all samples to the given reference fasta (H1N1pdm09_Cali09 full genome).\n",
    "\n",
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "from importlib.machinery import SourceFileLoader\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# load custom flu and ngs libraries \n",
    "laeb_lib = expanduser(\"../python_lib\") # folder where custom libraries are saved \n",
    "fc = SourceFileLoader('fc', \"%s/flu_common.py\"%(laeb_lib)).load_module()\n",
    "ngs = SourceFileLoader('ngs', \"%s/laeb_ngs_pipeline.py\"%(laeb_lib)).load_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs \n",
    "# file path to data folder - fastq files to be analysed must be in {data folder}/raw\n",
    "data_folder = './data' \n",
    "# reference fasta file name (should be placed in input_folder)\n",
    "ref_fasta_fname = './input/H1N1pdm09_Cali09.fasta' \n",
    "# CSV file containing the CDR regions of each gene segment (numbering should be based on that of the given reference sequence)\n",
    "cds_coords = \"./input/CDS_H1N1pdm09_Cali09.csv\"\n",
    "nucpos_shift = \"./input/CDS_shift_H1N1pdm09_Cali09.csv\"\n",
    "# file path to metadata file. \n",
    "meta_fname = './input/metadata.csv' \n",
    "\n",
    "# mapping options\n",
    "trimmomatic_fpath = expanduser('~/opt/anaconda3/pkgs/trimmomatic-0.39-1/share/trimmomatic-0.39-1/') # file path to trimmomatic\n",
    "threadnum = 4 # number of CPU threads for parallelization \n",
    "base_qual_threshold = 20 # minimum accepted base quality \n",
    "max_indel_prop = 0.05 # max tolerable proportion of indels wrt read length \n",
    "max_indel_abs = 10 # max tolerable absolute number of indels \n",
    "\n",
    "# variant calling options\n",
    "Query_HAnum_subtype = 'absH1pdm' # query HA numbering subtype (i.e. numbering based on CDR HA protein )\n",
    "HAnum_subtype = 'H3' # reporting HA numbering subtype\n",
    "subtype_ant = 'H1ant'  # HA canonical antigenic site of interest \n",
    "min_cov = 50 # minimum coverage \n",
    "min_var_freq = 0\n",
    "min_var_prop = 0.02 # minimum variant proportion \n",
    "err_tol = 0.01 # threshold to which variant called could result from base calling error \n",
    "min_breadth = 0.7 # min breadth of gene segment to be mapped for further analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and functions \n",
    "\n",
    "This cell perform several initialisation procedures, including: \n",
    " - defining parameters needed by the pipeline (e.g. gene segment length, etc.) and initialise to get CDR regions of each protein.\n",
    " - defining dataframe for HA numbering conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presets \n",
    "reffasta = ref_fasta_fname\n",
    "\n",
    "# initialise\n",
    "ngs = SourceFileLoader('ngs', \"%s/laeb_ngs_pipeline.py\"%(laeb_lib)).load_module()\n",
    "gene_to_proteinorf, influenza_gene_len, sorted_refnames, nucpos_shift = ngs.initialisation(cds_coords, reffasta, laeb_lib, nucpos_shift=nucpos_shift)\n",
    "display (gene_to_proteinorf.head())\n",
    "\n",
    "ha_numbering_conversion = pd.read_csv(expanduser('%s/HA_numbering_conversion.csv'%(laeb_lib)),\n",
    "                                      na_values='-')\n",
    "ha_numbering_conversion = ha_numbering_conversion.set_index(Query_HAnum_subtype)\n",
    "display (ha_numbering_conversion.head())\n",
    "\n",
    "all_bases = ['A', 'T', 'G', 'C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metadata\n",
    "\n",
    "Sample IDs are parsed from metadata file under the header \"sampid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# metadata \n",
    "## metadata must have 'sampid' header which is used as sample identifier \n",
    "meta_df = pd.read_csv('{}'.format(meta_fname))\n",
    "sorted_sampid = sorted(set(meta_df['sampid']))\n",
    "meta_df['date'] = pd.to_datetime(meta_df['date'])\n",
    "# round CT to nearest integer \n",
    "meta_df['ct'] = np.around(meta_df['CT'], 0)\n",
    "meta_df = meta_df.sort_values(by=['project', 'sampid']).set_index('sampid')\n",
    "display (meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to raw FASTQ files sorted by read direction \n",
    "dat_df = []\n",
    "\n",
    "for sampid in sorted_sampid: \n",
    "    for fname in os.listdir('{}/raw'.format(data_folder)): \n",
    "        if re.search('{}.gz'.format(sampid), fname): \n",
    "            dat_df.append({'sampid':sampid, 'fpath':'{}/raw/{}'.format(data_folder, fname)})#, 'read':read, })\n",
    "\n",
    "dat_df = pd.DataFrame.from_dict(dat_df)\n",
    "dat_df = dat_df.set_index(['sampid'])#, 'read'])\n",
    "dat_df = dat_df.sort_index()\n",
    "dat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform ```FASTQC```\n",
    "\n",
    "```FastQC``` checks the quality of the raw FASTQ files (i.e. $\\ge$90% of reads has above acceptable quality score), determine the crop length for ```trimmomatic``` and ensure that there are negligible amount of adapter sequences present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('{}/fastqc'.format(data_folder)): \n",
    "    os.mkdir('{}/fastqc'.format(data_folder))\n",
    "\n",
    "def analyse_fastqc(fname): \n",
    "    fhandle = open(fname, 'r').readlines()\n",
    "    module = ''\n",
    "    df_dict = {}\n",
    "    \n",
    "    for line in fhandle: \n",
    "        if re.search('>>Per base sequence quality', line): \n",
    "            module = 'pbsq' \n",
    "            continue\n",
    "        if re.search('>>Adapter Content', line): \n",
    "            # check whether there is adapter content \n",
    "            module = 'ac'\n",
    "            continue\n",
    "        if re.search('>>Per sequence quality scores', line):\n",
    "            module = 'psqs'\n",
    "            continue\n",
    "        if module != '' and re.search('>>END_MODULE', line):\n",
    "            df_dict[module] = pd.DataFrame(df)\n",
    "            module = ''\n",
    "            \n",
    "        if module != '':\n",
    "            if re.search('^#', line):\n",
    "                headers = re.sub('#', '', line.strip()).split('\\t')\n",
    "                df = {h:[] for h in headers}\n",
    "            else: \n",
    "                values = line.strip().split('\\t')\n",
    "                for _h, h in enumerate(headers):\n",
    "                    df[h].append(values[_h])\n",
    "\n",
    "    # % of reads with acceptable phred score > base_qual_threshold\n",
    "    df = df_dict['psqs']\n",
    "    df['Quality'] = pd.to_numeric(df['Quality'])\n",
    "    df['Count'] = pd.to_numeric(df['Count'])\n",
    "    percent_abv_qualthres = sum(df[df['Quality']>=base_qual_threshold]['Count'])/sum(df['Count'])\n",
    "    \n",
    "    # max position where median phred score > base_qual_threshold\n",
    "    df = df_dict['pbsq']\n",
    "    df['Median'] = pd.to_numeric(df['Median'])\n",
    "    \n",
    "    try: \n",
    "        start_pos = int(re.search('^(\\d+)-*', df[df['Median']>=base_qual_threshold]['Base'].iloc[0]).group(1))\n",
    "    except: \n",
    "        start_pos = None \n",
    "    try: \n",
    "        end_pos = int(re.search('-*(\\d+)$', df[df['Median']>=base_qual_threshold]['Base'].iloc[-1]).group(1))\n",
    "    except: \n",
    "        end_pos = None\n",
    "    \n",
    "    df = df_dict['ac'].drop('Position', axis=1)\n",
    "    ac = {}\n",
    "    for adapter in list(df):\n",
    "        val = np.sum(pd.to_numeric(df[adapter]))/len(df[adapter])\n",
    "        if adapter in ac: \n",
    "            if val > ac[adapter]: \n",
    "                ac[adapter] = val\n",
    "        else:\n",
    "            if val > 0.: \n",
    "                ac[adapter] = val \n",
    "    \n",
    "    return percent_abv_qualthres, ac, start_pos, end_pos\n",
    "    \n",
    "adapters_to_rm = {}\n",
    " \n",
    "for sampid in set(dat_df.index): \n",
    "    \n",
    "    # run fastqc \n",
    "    if not os.path.isfile('{}/fastqc/{}_fastqc.html'.format(data_folder, sampid)):\n",
    "        fpath = dat_df.loc[sampid]['fpath']\n",
    "        if re.search('Darwin', platform.platform()):\n",
    "            cmd = ['zcat', '<', fpath, '|', 'fastqc', 'stdin:{}'.format(sampid), '--outdir={}/fastqc'.format(data_folder)]\n",
    "        else: \n",
    "            cmd = ['zcat', fpath, '|', 'fastqc', 'stdin:{}'.format(sampid), '--outdir={}/fastqc'.format(data_folder)]\n",
    "        subprocess.call(' '.join(cmd), shell=True)\n",
    "\n",
    "        ## unzip files \n",
    "        cmd = ['unzip', '{}/fastqc/{}_fastqc.zip'.format(data_folder, sampid), \n",
    "               '-d', '{}/fastqc/'.format(data_folder)]\n",
    "        subprocess.call(cmd)\n",
    "        \n",
    "    # analyse results \n",
    "    percent_abv_qualthres, ac, start_pos, end_pos = analyse_fastqc('{}/fastqc/{}_fastqc/fastqc_data.txt'.format(data_folder, sampid))\n",
    "    if len(ac) > 0: \n",
    "        for k, v in ac.items(): \n",
    "            if k in adapters_to_rm: \n",
    "                if v > adapters_to_rm[k]: \n",
    "                    adapters_to_rm[k] = v\n",
    "            else: \n",
    "                if v > 0.: \n",
    "                    adapters_to_rm[k] = v\n",
    "    \n",
    "    dat_df.at[sampid, 'percent_abv_qualthres'] = percent_abv_qualthres\n",
    "    dat_df.at[sampid, 'start_pos'] = start_pos\n",
    "    dat_df.at[sampid, 'end_pos'] = end_pos\n",
    "\n",
    "if len(adapters_to_rm) > 0: \n",
    "    print ('- Presence of adapter sequence (max. proportion of reads) -')\n",
    "    for k, v in adapters_to_rm.items(): \n",
    "        print (\"{}: {:.2f}%\".format(k, v))\n",
    "\n",
    "dat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted trimmomatic scirpt for 454-sequencing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_min_len = 30\n",
    "maxinfo_target_len = 80\n",
    "maxinfo_strictness = 0.4\n",
    "adapter_seed_mismatch = 2\n",
    "adapter_fdat = fc.parsefasta('./input/454-emPCR.fa')\n",
    "\n",
    "if not os.path.isdir('{}/trimmed'.format(data_folder)): \n",
    "    os.mkdir('{}/trimmed'.format(data_folder))\n",
    "    \n",
    "def LTScore(l_array):\n",
    "    return 1/(1+np.exp(maxinfo_target_len-l_array))\n",
    "\n",
    "# maxinfo function of trimmomatic \n",
    "def maxinfo(length_array, quality_array, \n",
    "            target_len=maxinfo_target_len, strictness=maxinfo_strictness): \n",
    "    \n",
    "    LTscore = 1/(1+np.exp(target_len-length_array))\n",
    "    Covscore = length_array**(1-strictness)\n",
    "    \n",
    "    correctness_array = 1-(10**(-quality_array/10))\n",
    "    prod_array = np.array([np.prod(correctness_array[:l+1]) for l in range(len((length_array)))])\n",
    "    Errscore = prod_array**strictness\n",
    "    \n",
    "    return np.argmax(LTscore*Covscore*Errscore)+1\n",
    "\n",
    "def trim_reads_fn(sampid): \n",
    "    trimmed_output = open('{}/trimmed/{}_trimmed.fastq'.format(data_folder, sampid), 'w')\n",
    "    total_reads = 0 \n",
    "    discarded_reads = 0\n",
    "    original_reads_len = []\n",
    "    remaining_reads_len = []\n",
    "    \n",
    "    with gzip.open(dat_df.loc[sampid]['fpath'], \"rt\") as fhandle:\n",
    "        for rec in SeqIO.parse(fhandle, 'fastq'):\n",
    "            \n",
    "            sequence_list = list(rec.seq)\n",
    "            qarray = np.array(rec.letter_annotations[\"phred_quality\"])\n",
    "            \n",
    "            total_reads += 1\n",
    "            original_reads_len.append(len(sequence_list))\n",
    "            \n",
    "            # sequence fall below min_len\n",
    "            if len(sequence_list) < trim_min_len:\n",
    "                discarded_reads += 1\n",
    "                continue \n",
    "            \n",
    "            # cut adapter \n",
    "            adapter_header_to_match = []\n",
    "            for adapter_header, adapter_seq in adapter_fdat.items():\n",
    "                for idx in range(len(sequence_list)):\n",
    "                    # get segment of rec seq to align \n",
    "                    rec_seq_to_align = rec.seq[idx:idx+len(adapter_seq)]\n",
    "                    match = sum(c1==c2 for c1,c2 in zip(adapter_seq, rec_seq_to_align))\n",
    "                    # continue if no match scores \n",
    "                    if match == 0: \n",
    "                        continue \n",
    "                    # make sure mismatch is below threshold \n",
    "                    if len(rec_seq_to_align) - match <= adapter_seed_mismatch:\n",
    "                        adapter_header_to_match.append({'adapter_header':adapter_header, \n",
    "                                                        'idx':idx, \n",
    "                                                        'match':match, \n",
    "                                                        'seqlen':len(rec_seq_to_align)})\n",
    "            if len(adapter_header_to_match) > 0: \n",
    "                adapter_header_to_match = pd.DataFrame.from_dict(adapter_header_to_match)\n",
    "                likely_adapter_match = adapter_header_to_match.iloc[adapter_header_to_match['match'].idxmax()]\n",
    "                \n",
    "                sequence_list = sequence_list[:likely_adapter_match.idx]\n",
    "                qarray = qarray[:likely_adapter_match.idx]\n",
    "                              \n",
    "            # cut leading and trailing \n",
    "            leadtrail_idx = np.argwhere(qarray<3).T[0]\n",
    "            # start with trailing  \n",
    "            if len(leadtrail_idx) > 0:\n",
    "                if leadtrail_idx[-1]+1 == len(sequence_list): \n",
    "                    for _i, idx in enumerate(leadtrail_idx[::-1]): \n",
    "                        if _i == 0: \n",
    "                            prev_idx = idx \n",
    "                        else: \n",
    "                            if prev_idx-idx > 1:  \n",
    "                                sequence_list = sequence_list[:prev_idx]\n",
    "                                qarray = qarray[:prev_idx]\n",
    "                                break \n",
    "                            else: \n",
    "                                prev_idx = idx \n",
    "                # then with leading \n",
    "                if leadtrail_idx[0] == 0: \n",
    "                    for _i, idx in enumerate(leadtrail_idx): \n",
    "                        if _i == 0: \n",
    "                            prev_idx = idx \n",
    "                        else: \n",
    "                            if idx-prev_idx  > 1:  \n",
    "                                sequence_list = sequence_list[prev_idx+1:]\n",
    "                                qarray = qarray[prev_idx+1:]\n",
    "                                break \n",
    "                            else: \n",
    "                                prev_idx = idx \n",
    "            \n",
    "            # sequence fall below min_len after all the trimming \n",
    "            if len(sequence_list) < trim_min_len:\n",
    "                discarded_reads += 1\n",
    "                continue \n",
    "            \n",
    "            # cut to maxinfo crop  \n",
    "            maxinfo_crop = maxinfo(np.linspace(1, len(sequence_list), len(sequence_list)), qarray)\n",
    "            sequence_list = sequence_list[:maxinfo_crop]\n",
    "            qarray = qarray[:maxinfo_crop]\n",
    "            \n",
    "            # cut to crop length \n",
    "            trim_crop = int(dat_df.loc[sampid]['end_pos'])\n",
    "            sequence_list = sequence_list[:trim_crop]\n",
    "            qarray = qarray[:trim_crop]\n",
    "            \n",
    "            # sequence fall below min_len after all the trimming \n",
    "            if len(sequence_list) < trim_min_len:\n",
    "                discarded_reads += 1\n",
    "                continue \n",
    "            \n",
    "            # write to file \n",
    "            seq_line = \"\".join(sequence_list)\n",
    "            qual_line = \"\".join(list(map(chr, list(qarray+33))))\n",
    "            trimmed_output.write('@{}\\n{}\\n+\\n{}\\n'.format(rec.id, seq_line, qual_line))\n",
    "            remaining_reads_len.append(len(seq_line))\n",
    "    \n",
    "    trimmed_output.close()\n",
    "    \n",
    "    # gzip trimmed file \n",
    "    cmd = ['gzip', '{}/trimmed/{}_trimmed.fastq'.format(data_folder, sampid)]\n",
    "    subprocess.call(cmd)\n",
    "    \n",
    "    # update trimmed_stats \n",
    "    trimmed_stats.append({'sampid':sampid, 'total_reads':total_reads, 'discarded':discarded_reads, \n",
    "                          'initial_mean_len':np.mean(original_reads_len), 'initial_sd_len':np.std(original_reads_len, ddof=1), \n",
    "                          'trimmed_mean_len':np.mean(remaining_reads_len), 'trimmed_sd_len':np.std(remaining_reads_len, ddof=1)})\n",
    "\n",
    "if os.path.isfile(\"{}/trim_stats.csv\".format(data_folder)):\n",
    "    trimmed_stats = pd.read_csv(\"{}/trim_stats.csv\".format(data_folder))\n",
    "else: \n",
    "    # create shared list \n",
    "    manager = mp.Manager()\n",
    "    trimmed_stats = manager.list()\n",
    "\n",
    "    # parallelise across all sampids \n",
    "    pool = mp.Pool(processes=threadnum)\n",
    "    results = [pool.apply_async(trim_reads_fn, args=[sampid]) for sampid in sorted_sampid]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    trimmed_stats = pd.DataFrame.from_dict(trimmed_stats)\n",
    "    trimmed_stats['retained'] = trimmed_stats['total_reads']-trimmed_stats['discarded']\n",
    "    trimmed_stats['retained_prop'] = trimmed_stats['retained']/trimmed_stats['total_reads']\n",
    "    trimmed_stats = trimmed_stats[[\"sampid\", \"total_reads\", \"discarded\", \"retained\", \"retained_prop\",  \"initial_mean_len\", \"initial_sd_len\", \"trimmed_mean_len\", \"trimmed_sd_len\"]]\n",
    "    trimmed_stats.to_csv(\"{}/trim_stats.csv\".format(data_folder), index=False)\n",
    "\n",
    "trimmed_stats = trimmed_stats.set_index('sampid')\n",
    "trimmed_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read mapping\n",
    "\n",
    "```bowtie2``` to align the trimmed, merged reads to the reference sequence. \n",
    "\n",
    "Flags used for ```bowtie2```: \n",
    "```\n",
    "-x <refid> : Reference sequence to align by \n",
    "-X <int>   : If -X 100, a two 20-bp alignment + 60-bp gap would be valid but not if there is 61-bp gap \n",
    "-k <int>   : Searches for at most <int> of valid, distinct alignment for each read \n",
    "--local    : Does not require that the entire read align from one end to the other. Rather, some characters may be omitted (\"soft clipped\") from the ends in order to achieve the greatest possible alignment score. \n",
    "--very-sensitive : Same as -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Index reference sequence...')\n",
    "ref_key = re.sub('(^.+/|\\.[^\\.+]$)', '', reffasta)\n",
    "cmd = ['bowtie2-build', reffasta, ref_key] # map to barcode ref fasta\n",
    "subprocess.call(cmd)\n",
    "\n",
    "# align sequences \n",
    "print ('Mapping reads with bowtie2...')\n",
    "if not os.path.isdir(\"{}/align\".format(data_folder)):\n",
    "    os.mkdir(\"{}/align\".format(data_folder))\n",
    "\n",
    "for sampid in sorted_sampid:  \n",
    "    if not os.path.isfile('{}/align/{}.sam.gz'.format(data_folder, sampid)):\n",
    "        # mapping with bowtie\n",
    "        with open('./data/bt_aln.log', 'a') as output: \n",
    "            output.write('{}\\n'.format(sampid))\n",
    "            cmd = ['bowtie2', \n",
    "                   '-x', ref_key, \n",
    "                   '-X', str(max(influenza_gene_len.values())),\n",
    "                   '-k', '2', \n",
    "                   '--very-sensitive-local',\n",
    "                   '-p', str(threadnum), \n",
    "                   '-U', '{}/trimmed/{}_trimmed.fastq.gz'.format(data_folder, sampid),\n",
    "                   '-S', '{}/align/{}.sam'.format(data_folder, sampid)]\n",
    "            subprocess.call(cmd, stderr=subprocess.STDOUT, stdout=output)\n",
    "            output.write('\\n')\n",
    "        \n",
    "        # gzip sam file \n",
    "        cmd = ['gzip', '{}/align/{}.sam'.format(data_folder, sampid)]\n",
    "        subprocess.call(cmd)\n",
    "            \n",
    "print ('...done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse SAM files \n",
    "\n",
    "Quality filters:  \n",
    "- excluded all unmapped and non-primary read alignments\n",
    "- accept only bases with Q-score $\\ge$ ```base_qual_threshold``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_stats = ngs.parse_sam(sorted_sampid, sorted_refnames, data_folder, \n",
    "                              base_qual_threshold, max_indel_abs, max_indel_prop, \n",
    "                              nucpos_shift=nucpos_shift, threadnum=1, plt_show=0)\n",
    "display (mapping_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tally base and codon counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngs = SourceFileLoader('ngs', \"%s/laeb_ngs_pipeline.py\"%(laeb_lib)).load_module()\n",
    "ngs.tally_bases(sorted_sampid, threadnum=threadnum, reanalyze_bool=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant calling\n",
    "\n",
    "Other than the 2% frequency threshold, as per  (Illingworth, Bioinformatics, 2016), we compute a statistical threshold for a variant to be called. Suppose $q$ is  the minimum required base quality score, error rate will then be $p_e = 10^{-q/10}$. As such, if $n$ out of $N$ bases are called to a site, the probability that this event resulted from errors is modelled as: \n",
    "\n",
    "$p_{Err} = \\sum_{i=n}^{N}{\\begin{pmatrix}\n",
    "N \\\\\n",
    "i \n",
    "\\end{pmatrix}p_{e}^{i}(1-p_e)^{N-i}}$\n",
    "\n",
    "Variant is only called if $p_{Err}<0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variant_call_df = ngs.variant_calling(sorted_sampid, sorted_refnames, base_qual_threshold,\n",
    "                                      min_cov, min_var_prop, gene_to_proteinorf, err_tol, \n",
    "                                      ha_numbering_conversion=ha_numbering_conversion, \n",
    "                                      HAnum_subtype=HAnum_subtype, threadnum=threadnum)\n",
    "display (variant_call_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot nucleotide coverage plots for each subject\n",
    "\n",
    "Here, we plot the average coverage of all gene segments per sample in bins of 50bp. We also compute the breadth of coverage for each gene segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standardise maximum y-value for plots \n",
    "ymax = -1\n",
    "for sampid in sorted_sampid: \n",
    "    try: \n",
    "        map_nuc_results = pd.read_csv('./results/map_nuc_results_{}.csv'.format(sampid))\n",
    "    except: \n",
    "        continue\n",
    "    if map_nuc_results['Coverage'].max() > ymax: \n",
    "        ymax = map_nuc_results['Coverage'].max()\n",
    "ymax = 10**int(np.ceil(np.log10(ymax)))\n",
    "\n",
    "# average coverage is based on a sliding window of 50 bp with stepsize of 25 bp\n",
    "sliding_window=50\n",
    "stepsize=25\n",
    "label_size = 12\n",
    "color_scheme = [\"#2f4f4f\",\"#228b22\",\"#7f0000\",\"#000080\",\"#ff8c00\",\"#ffff00\",\"#00ff00\",\"#00ffff\",\"#ff00ff\",\"#1e90ff\",\"#ffe4b5\",\"#ff69b4\"]\n",
    "\n",
    "# array of sorted segment length \n",
    "sorted_gene_len = np.array([influenza_gene_len[refname] for refname in sorted(influenza_gene_len.keys())])\n",
    "\n",
    "# reindex meta_df based on subject_id and enrollment day \n",
    "meta_df = meta_df.reset_index().set_index(['subject_id', 'enrolD', 'sampid']).sort_index() \n",
    "\n",
    "# dataframe to plot overall distribution across all patients\n",
    "overall_gene_coverage_distribution = [] \n",
    "\n",
    "for subject_id in sorted(set(meta_df.index.get_level_values(0))): \n",
    "    print (subject_id)\n",
    "    \n",
    "    subject_meta_df = meta_df.loc[subject_id]\n",
    "    \n",
    "    # initialise coverage plot figure for each subject \n",
    "    with plt.style.context(\"default\"):\n",
    "        fig = plt.figure(figsize=(11.7, 4.1))#, constrained_layout=True)\n",
    "        spec = gridspec.GridSpec(1, 8, figure=fig, wspace=0.2, \n",
    "                                 width_ratios=sorted_gene_len/np.sum(sorted_gene_len))\n",
    "\n",
    "        axes = [] # list of subplots (by segments)\n",
    "        first_sample_bool = 1\n",
    "\n",
    "        for enrolD in sorted(set(subject_meta_df.index.get_level_values(0))): \n",
    "            enrolD_subject_meta_df = subject_meta_df.loc[enrolD]\n",
    "            #display (enrolD_subject_meta_df)\n",
    "            \n",
    "            for sampid in enrolD_subject_meta_df.index:\n",
    "                sampid_enrolD_subject_meta_df = enrolD_subject_meta_df.loc[sampid]\n",
    "                \n",
    "                try: \n",
    "                    timepoint_label = \"D%i (%s)\"%(int(sampid_enrolD_subject_meta_df.timepoint), sampid)\n",
    "                except: \n",
    "                    timepoint_label = \"T%i (%s)\"%(enrolD, sampid) \n",
    "\n",
    "                #sampid = enrolD_subject_meta_df['sampid']\n",
    "                sample_type = sampid_enrolD_subject_meta_df['SampleType']\n",
    "\n",
    "                # read map_nuc_results \n",
    "                if os.path.isfile('./results/map_nuc_results_{}.csv'.format(sampid)): \n",
    "                    # parse coverage/site quality results    \n",
    "                    map_nuc_results = pd.read_csv('./results/map_nuc_results_{}.csv'.format(sampid), keep_default_na=False)\n",
    "                else: \n",
    "                    print ('No mapped reads found for %s'%sampid)\n",
    "                    continue \n",
    "\n",
    "                for _r, refname in enumerate(sorted(influenza_gene_len.keys())):\n",
    "\n",
    "                    rdf = map_nuc_results[map_nuc_results['Gene']==refname]\n",
    "\n",
    "                    refseq_len = influenza_gene_len[refname]\n",
    "                    gene_start_pos = 1\n",
    "                    gene_end_pos = gene_start_pos+refseq_len\n",
    "\n",
    "                    # step size \n",
    "                    x_values = np.arange(gene_start_pos, gene_end_pos, stepsize)\n",
    "                    if x_values[-1] != gene_end_pos: \n",
    "                        x_values = np.append(x_values, gene_end_pos)\n",
    "\n",
    "                    # compute mean coverage over sliding_window sized bins \n",
    "                    y_values = np.zeros(len(x_values)-1)\n",
    "                    mapdf = rdf[['Position', 'Coverage']].set_index('Position').sort_index()\n",
    "\n",
    "                    # compute breadth of coverage \n",
    "                    breadth = []\n",
    "                    for idx, x_val in enumerate(x_values): \n",
    "                        if idx == 0:\n",
    "                            continue \n",
    "\n",
    "                        pos_range = range(int(np.max([0., x_val-sliding_window])), x_val)\n",
    "\n",
    "                        # compute mean coverage over 200bp bins \n",
    "                        try:\n",
    "                            mean_coverage = np.mean(mapdf.loc[pos_range])['Coverage']\n",
    "                        except: \n",
    "                            mean_coverage = np.zeros(len(pos_range))\n",
    "                            for _p, p in enumerate(pos_range): \n",
    "                                try: \n",
    "                                    mean_coverage[_p] = mapdf.loc[p]['Coverage']\n",
    "                                except: \n",
    "                                    continue \n",
    "                            mean_coverage = np.mean(mean_coverage)\n",
    "\n",
    "                        y_values[idx-1] = mean_coverage\n",
    "\n",
    "                        # breadth of coverage \n",
    "                        if mean_coverage >= min_cov: \n",
    "                            breadth += range(x_values[idx-1], x_val)\n",
    "\n",
    "                        # store computed mean coverage \n",
    "                        overall_gene_coverage_distribution.append({'gene':refname, 'pos':x_val, 'coverage':mean_coverage})\n",
    "\n",
    "\n",
    "                    # compute breadth of coverage \n",
    "                    meta_df.loc[(subject_id, enrolD, sampid), refname] = len(breadth)/refseq_len\n",
    "\n",
    "                    \"\"\"\n",
    "                    num_overlapping_polymorphic_sites = len(set(all_polymorphic_sites.loc[refname, 'nucpos'])&set(rdf[rdf['Coverage']>=min_cov]['Position']))\n",
    "                    meta_df.loc[(subject_id, enrolD), refname] = num_overlapping_polymorphic_sites/len(all_polymorphic_sites.loc[refname, 'nucpos']) \n",
    "                    \"\"\"\n",
    "\n",
    "                    if first_sample_bool == 1:\n",
    "                        # add subplot for 1st sample \n",
    "                        ax = fig.add_subplot(spec[0,_r])\n",
    "                        ax.set_title(refname, fontsize=label_size) # title \n",
    "\n",
    "                        # plot min_cov line\n",
    "                        ax.plot(x_values[1:], \n",
    "                                np.zeros(len(x_values)-1)+min_cov, \n",
    "                                color='k', linestyle='--')\n",
    "                        axes.append(ax)\n",
    "                    else: \n",
    "                        ax = axes[_r]\n",
    "\n",
    "                    # plot coverage \n",
    "                    # patch samples\n",
    "                    if re.search('_P$', sampid):\n",
    "                        label = '{}-{} (patch)'.format(timepoint_label, sample_type)\n",
    "                        ax.plot(x_values[1:],\n",
    "                                y_values, '--',\n",
    "                                color=color_scheme[enrolD-1],\n",
    "                                label=label)\n",
    "                    else: \n",
    "                        label = '{}{}'.format(timepoint_label, \"\" if pd.isna(sample_type) else \"-%s\"%(sample_type))\n",
    "                        ax.plot(x_values[1:],\n",
    "                                y_values, \n",
    "                                color=color_scheme[enrolD-1],\n",
    "                                label=label)\n",
    "\n",
    "                if first_sample_bool == 1:\n",
    "                    first_sample_bool = 0\n",
    "\n",
    "        for _ax, ax in enumerate(axes):\n",
    "            if _ax == 0: \n",
    "                ax.set_ylabel('Coverage')\n",
    "                ax.yaxis.label.set_fontsize(label_size)\n",
    "            else: \n",
    "                # remove y-axis label (sharey)\n",
    "                ax.tick_params(labelleft=False)\n",
    "\n",
    "            # gray facecolor for odd panels \n",
    "            if (_ax%2 != 0): \n",
    "                ax.set_facecolor(color='#d1d1d1')\n",
    "\n",
    "            # remove left and right spines \n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "\n",
    "            # set xlim and xtick labels\n",
    "            refname = sorted(influenza_gene_len.keys())[_ax]\n",
    "            refseq_len = influenza_gene_len[refname]\n",
    "            gene_start_pos = 1\n",
    "            gene_end_pos = gene_start_pos+refseq_len\n",
    "\n",
    "            if refseq_len > 2000: \n",
    "                ax.set_xticks(np.linspace(gene_start_pos,  gene_end_pos-1, 4))\n",
    "                ax.set_xticklabels(map(int, np.linspace(gene_start_pos,  gene_end_pos-1, 4)))\n",
    "            elif refseq_len > 1000: \n",
    "                ax.set_xticks(np.linspace(gene_start_pos,  gene_end_pos-1, 3))\n",
    "                ax.set_xticklabels(map(int, np.linspace(gene_start_pos,  gene_end_pos-1, 3)))\n",
    "            else: \n",
    "                ax.set_xticks(np.linspace(gene_start_pos, gene_end_pos-1, 2))\n",
    "                ax.set_xticklabels(map(int, np.linspace(gene_start_pos,  gene_end_pos-1, 2)))\n",
    "\n",
    "            # set ylim and yscale \n",
    "            ax.set_ylim((1, ymax))\n",
    "            ax.set_yscale('symlog')\n",
    "\n",
    "            # change tick size \n",
    "            ax.tick_params(axis='both', which='major', labelsize=label_size*0.8)\n",
    "\n",
    "            # change axis size \n",
    "            ax.xaxis.label.set_fontsize(label_size)\n",
    "\n",
    "        # x-axis label \n",
    "        fig.text(0.5, 0.01, 'Position', ha='center', fontsize=label_size)\n",
    "        plt.legend(loc='center left',  bbox_to_anchor=(1, 0.5))\n",
    "        #plt.tight_layout()\n",
    "        plt.savefig('./results/figures/coverage_plots_{}.pdf'.format(subject_id.replace(\"/\", \"-\")), \n",
    "                    bbox_inches='tight', pad_inches=0.)\n",
    "        plt.show()\n",
    "\n",
    "# convert overall_gene_coverage_distribution to dataframe \n",
    "overall_gene_coverage_distribution = pd.DataFrame.from_dict(overall_gene_coverage_distribution)\n",
    "overall_gene_coverage_distribution = overall_gene_coverage_distribution.set_index(['gene', 'pos']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall coverage across all patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with plt.style.context(\"default\"):\n",
    "    # initialise coverage plot figure \n",
    "    fig = plt.figure(figsize=(11.7, 4.1))#, constrained_layout=True)\n",
    "    spec = gridspec.GridSpec(1, 8, figure=fig, wspace=0.2, \n",
    "                             width_ratios=sorted_gene_len/np.sum(sorted_gene_len))\n",
    "\n",
    "    axes = [] # list of subplots (by segments)\n",
    "\n",
    "    # add subplot for 1st sample \n",
    "    for _r, refname in enumerate(sorted_refnames): \n",
    "        ax = fig.add_subplot(spec[0,_r])\n",
    "        ax.set_title(refname, fontsize=label_size) # title \n",
    "\n",
    "        # plot min_cov line\n",
    "        Y_array = []\n",
    "        X_array = np.array(sorted(set(overall_gene_coverage_distribution.loc[refname].index)))\n",
    "        for x_val in X_array:\n",
    "            Y_array.append(np.array(overall_gene_coverage_distribution.loc[(refname, x_val), 'coverage']))\n",
    "        Y_array = np.array(Y_array)\n",
    "\n",
    "        ax.plot(X_array, [50]*len(X_array), \"--\", color='#fcae91')\n",
    "\n",
    "        mu = np.median(Y_array, axis=1)\n",
    "        ax.plot(X_array, mu, color='#000000')\n",
    "        ax.fill_between(X_array, np.quantile(Y_array, 0.25, axis=1), \n",
    "                        np.quantile(Y_array, 0.75, axis=1), facecolor='#ef3b2c', alpha=0.5)\n",
    "        ax.fill_between(X_array, np.min(Y_array, axis=1), \n",
    "                        np.max(Y_array, axis=1), facecolor='#fcbba1', alpha=0.2)\n",
    "\n",
    "        axes.append(ax)\n",
    "\n",
    "    for _ax, ax in enumerate(axes):\n",
    "        if _ax == 0: \n",
    "            ax.set_ylabel('Coverage')\n",
    "            ax.yaxis.label.set_fontsize(label_size)\n",
    "        else: \n",
    "            # remove y-axis label (sharey)\n",
    "            ax.tick_params(labelleft=False)\n",
    "\n",
    "        # gray facecolor for odd panels \n",
    "        if (_ax%2 != 0): \n",
    "            ax.set_facecolor(color='#d1d1d1')\n",
    "\n",
    "        # remove left and right spines \n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        # set xlim and xtick labels\n",
    "        refname = sorted(influenza_gene_len.keys())[_ax]\n",
    "        refseq_len = influenza_gene_len[refname]\n",
    "        gene_start_pos = 1\n",
    "        gene_end_pos = gene_start_pos+refseq_len\n",
    "\n",
    "        if refseq_len > 2000: \n",
    "            ax.set_xticks(np.linspace(gene_start_pos,  gene_end_pos-1, 4))\n",
    "            ax.set_xticklabels(map(int, np.linspace(gene_start_pos,  gene_end_pos-1, 4)))\n",
    "        elif refseq_len > 1000: \n",
    "            ax.set_xticks(np.linspace(gene_start_pos,  gene_end_pos-1, 3))\n",
    "            ax.set_xticklabels(map(int, np.linspace(gene_start_pos,  gene_end_pos-1, 3)))\n",
    "        else: \n",
    "            ax.set_xticks(np.linspace(gene_start_pos, gene_end_pos-1, 2))\n",
    "            ax.set_xticklabels(map(int, np.linspace(gene_start_pos,  gene_end_pos-1, 2)))\n",
    "\n",
    "        # set ylim and yscale \n",
    "        ax.set_ylim((1, ymax))\n",
    "        ax.set_yscale('symlog')\n",
    "\n",
    "        # change tick size \n",
    "        ax.tick_params(axis='both', which='major', labelsize=label_size*0.8)\n",
    "\n",
    "        # change axis size \n",
    "        ax.xaxis.label.set_fontsize(label_size)\n",
    "\n",
    "    # x-axis label \n",
    "    fig.text(0.5, 0.01, 'Position', ha='center', fontsize=label_size)\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig('./results/figures/coverage_plots_overall.pdf', \n",
    "                bbox_inches='tight', pad_inches=0.)\n",
    "    plt.show()\n",
    "\n",
    "# save meta_df with coverage breadth to results \n",
    "meta_df.to_csv('./results/metadata_w_covbreadth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
